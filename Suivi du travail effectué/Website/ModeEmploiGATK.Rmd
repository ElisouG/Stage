---
title: "Mode d'emploi des scripts GATK"
output:
   html_document: 
    fig_caption: yes
    highlight: zenburn
    theme: cerulean
    toc: yes
    toc_depth: 5
    toc_float: yes
---

* * * * *

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Liste des nécessités pour faire fonctionner les scripts

Il est nécessaire d'avoir les logiciels suivants : gatk.jar, cromwell.jar et wdltool.jar. Pour les faire tourner il est également nécessaire d'avoir Java 8. Cromwell a besoin d'1 fichier config pour tourner correctement.

# Pré-requis

Pour obtenir les fichiers d'index des fichiers bam, il est nécessaire d'installer la suite d'outils Samtools. Il faut ensuite taper la commande suivante dans un terminal, là où se situe les fichier bam : $ samtools index fichier.bam fichier.bam.bai .

Pour écrire le fichier d'input au format texte, il faut taper le nom de l'individus (ou de l'échantillon) puis faire tab puis taper le chemin d'accès du fichier bam puis faire tab puis taper le chemin d'accès du fichier bam.bai en faisant un individu par ligne.

# Important

* Changer les mails dans le début du script sge.    
* Vérifier tous les chemins inscrits au sein de tous les fichiers.    
* Vérifier que vous avez tous les fichiers et l'architecture suivante:     

![Architecture de fichiers nécessaire](Architecture_files_cluster_GATK.PNG)

# Les données

Les données proviennent d'un séquençage suivi d'un alignement sur le génome qui produit des fichiers bam.

# Les scripts : Pipeline de génotypage

Ce pipeline est décomposé en 4 phases distinctes qui utilisent différentes resources et qui permettent de faire de la parallèlisation pour optimiser les temps de calculs. Ces 4 phases sont les suivantes :

* Alignement des données (réalisé par l'équipe de la plateforme MBB);   
* Nettoyage des données;    
* Découverte des variants;   
* Evaluation des variants.    

![Vue générale du pipeline de génotypage](vue générale phases.PNG)

Ce pipeline est une collection de script wdl, bash et json à exécuter dans l'ordre sur une machine Linux avec une gestion des jobs de type gridengine comme un cluster de calcul. Il est également exécutable sur une machine Linux sans gestion des jobs tels qu'une station de travail.

Pour rendre les données analysables pour la découverte de variants quelques étapes des phases précédentes nécessaires sont détaillées ci-après. 

Il existe un script qui lance les jobs sur le cluster mbb. "LancerScript.sge"

## Data Pre-processing : Nettoyage des données

### Lancement du script

Enlever le # avant la ligne contenant java . Enregister. Taper dans le terminal : qsub LancerScript.sge

### Explications

Le nettoyage des données est une phase nécessaire pour pouvoir réaliser la suite des analyses. En effet, elle permet d'obtenir des reads calibrés nécessaire à la phase 3. Cette phase comprend des étapes réalisées par Picard ou par GATK.     
![Phase de nettoyage des donnees](NettoyageDonnees.PNG)    

La première étape de cette phase  appelée "ReorderSam" est réalisée par Picard. Cette étape range les reads en fonction de leur appartenance à un groupe de liaison dans l'ordre des contigs du génome de référence. En effet, si les contigs ne sont pas rangés dans le même ordre cela pose problème à GATK. C'est pour pallier à ce problème que l'on réalise cette étape. Elle est notamment nécessaire lorsque l'alignement a utilisé un ordre différents des groupes de liaisons.

La seconde étape nommée "Markduplicates" utilise Picard. Cette étape est importante car elle marque puis élimine les duplicats de séquençage. Ces duplicas sont des artéfacts dû à la PCR d'enrichissement des banques qui est réalisée en amont du séquençage. Ils sont retirés pour éviter que ces reads soient considérés comme des morceaux de transcrits plus représentés que d'autres.

La troisième étape intitulée "SortSam" est également réalisée par Picard. Cette étape range de nouveau les reads car certains ont été supprimés (masqués) par l'étape précédente. Cela permet d eles mettre en fin de fichier pour ne pas qu'il soit pris en compte dans l'étape qui suit.

Les deux étapes qui vont suivre sont nécessaires car les algorithmes de découverte de variants utilisent la qualité des bases de chaque reads pour déterminer s'il s'agit bel et bien d'une variation ou d'un artéfact. En effet, les scores de qualité sont dépendants du nombre de reads présents. Or certains reads considérés comme des duplicats de séquençage ont été retirés. Il ne faut donc plus les prendre en compte. D'autre part, à ces scores de qualité sont appliqués des erreurs systématiques qui  doivent être recalibrés. POur faire cela GATK utilise un algorithmle de "machine-learning" pour modeller empiriquement ces erreurs et ajuster les scores de qualité. Ces deux étapes sont "BaseRecalibrator"et "ApplyBQSR". La première est réalisée 2 fois afin de visualiser l'efficacité de la recalibration des reads.  Cette étape crée une table qui sera utilisée pour la recalibration. La seconde ("ApplyBQSR") utilise donc la table de recalibration produite par l'étape précédente pour effectuer la recalibration de chacune des bases de chaque reads.

La dernière étape nommée "AnalyzeCovariates" eégalement effectuée par GATK, produit des graphiques montrant l'efficacité de cette recalibration.

## Data Variant Calling : Découverte de variants

### Lancement du script

Enlever le # avant la ligne contenant java . Enregister. Taper dans le terminal : qsub LancerScript.sge

### Explications

Cette phase est entièrement réalisée par GATK et est la phase clé puisque c'est elle qui extrait les génotypes de chacun des individus. Le succès de cette phase dépend à la fois de la minimisation des Faux positifs et à la fois des Faux négatifs. POur faire cela, GATK procède en plusieurs étapes: la découverte de variants (étape "HaplotypeCaller") par individus, la fusion des génotypes de chaque individus (étapes "CombineGVCFs" et "GenotypeGVCFs") par lignée puis la filtration des variants par type de variations (SNPs ou INDELs). Les deux premières étapes ont été designés pour maximiser la sensibilité alors que la filtration permet de maximiser la spécificité par un choix de filtre adaptables à chaque jeu de données. Bien entendu, la découverte de variants dépends du type de données (Whole genome, transcriptome, exome, etc.) mais aussi de d'autres paramètres provenant du séquençage comme la couverture ou la profondeur de séquençage.
![Phase de découverte de variants](DecouverteVariants.PNG)

La première étape "HaplotypeCaller" réalise en même temps la recherche de SNPs et d'INDELs par un ré-assemblage *de novo* des haplotypes des séquences actives de la région ciblée. Cet outils est capable de détecter 5 types de variations différentes. [Source1](https://software.broadinstitute.org/gatk/documentation/article?id=3682) Il s'agit des SNPs, MNPs, INDELs, Mixed, et Symbolic. Les SNPs représentent des "single nucleotide polymorphism". Les MNPs représentent les "multi-nucleotide polymorphism". Les INDELs représentent des évènements d'insertions ou de délétions de nucléotides. Les Mixed représentent une combinaison de SNPs et d'INDELs à une seule et même position. Les Symbolic montrent qu'il se passe quelque chose à cette position mais qu'on ne sait pas exactement ce qu'il représente. Parfois, HaplotypeCaller utilise l'allèle "non ref" ou * pour signifier la présence d'une suppression étendue ou des évènements indéfinis comme un trés grand allèle, la perte d'un seul allèle, la perte de deux allèles, etc. [Source 2](https://software.broadinstitute.org/gatk/documentation/article.php?id=6926) HaplotypeCaller produit un fichier au format gVCF (Variant Calling Format) pour chaque individus.

La seconde étape "CombineGVCFs" combine en un seul fichier VCF tous les gVCFs produits dans l'étape précédente.

La troisième étape "GenotypeGVCFs" utilise le fichier VCF de tous les gVCFs combinés de chaque individus pour faire un recalibrage des scores de qualité des variants. Cette analyse réalisée par lignée permet la détection de variants au niveau des locus complexes. Il s'agit d'une aggrégation conjointe multi-échantillons qui fusionnent les enregistrement de manière sophistiquée : pour chaque position du fichier VCF (gVCFs combinés), cet outils combinent tous les enregistrment couvrant le même position, produit des probabilités de génotype correct, re-génotype l'enregistrement nouvellement fusionné puis le ré-annote.

La quatrième étape "SelectSNPs" ou "SelectINDELs" est la sélection des variations selon leur type (SNPs ou INDELs). Cette étape va permettre de séparer les variants afin de leur appliquer des filtres.

La cinquième étape "hardfilterSNPs" ou "hardfilterINDELs" est l'application de filtres.En effet, selon les bonnes pratiques de découverte de Variants à partir de données de RNA-seq, il faut réaliser un "hard-filtering". C'est-à-dire choisir des valeurs seuils qui définissent quelles données sont considérées comme correcte de celles qui ne le sont pas. Pour cela, ils recommandent différentes valeurs pour différents champs en fonction du type de variation (SNPs ou Indels). Ces valeurs sont présentées dans le tableau suivant :

| Sigle du champs utilisé | Valeur si SNPs | Valeur si INDELs |
| :---------------------: | :------------: | :--------------: |
| QD | <2.0 | <2.0 |
| FS | >60.0 | >200.0 |
| SOR | >3.0 | >10.0 |
| ReadPosRankSum | <-8.0 | <20.0 |
| MQ | <40.0 |   |
| MQRankSum | <-12.5 |   |
| InbreedingCoeff |   | <-0.8 |

Ces différents sigles représentent différentes annotation qui signifient :

+ **Quality by depth (QD)** :  Cette annotation met en perspective le score QUAL du variant en la normalisant par la couverture disponible. Comme chaque reads contribue un peu au score QUAL, les variants dans les régions avec une couverture profonde peuvent avoir des scores QUAL artificiellement gonflés, donnant l'impression que cette variation est soutenue par plus de preuves qu'elle ne l'est réellement. Pour compenser cela, une normalisation de la qualité par la profondeur est effectué, ce qui donne une image plus objective de la qualité de la variation.    
+ **Fischer's Strand bias (FS)** : Le biais de brin est un type de biais de séquençage dans lequel un brin d'ADN est favorisé par rapport à l'autre, ce qui peut entraîner une évaluation incorrecte de la quantité de preuve observée pour un allèle par rapport à l'autre. L'annotation Fisher's Strand est l'une des méthodes qui vise à évaluer s'il y a un biais de brin dans les données. Il utilise le test exact de Fisher pour déterminer s'il existe un biais de brin entre les brins sens et anti-sens pour l'allèle de référence ou l'allèle alternatif. La sortie est une p-valeur en Phred. Plus la valeur obtenue est élevée, plus il y a un risque de biais. Et si il y a un biais cela indique la présence de faux positifs.    
+ **Strand Odds Ratio (SOR)** : C'est une autre façon d'estimer le biais de brin en utilisant un test similaire. Le SOR a été créé parce que le FS tend à pénaliser les variations qui se produisent aux extrémités des exons. Les reads issus des extrémités des exons tendent à n'être couverts que par des reads d'une seule direction et le FS donne un mauvais score à ces variations. Le SOR  quantt à lui prend en compte les ratios de reads qui couvrent les deux allèles.    
+ **ReadPosRankSum Rank Sum Test for relative positioning of REF versus ALT alleles within reads (ReadPosRankSum)** : Cette annotation teste s'il existe des preuves de biais dans la position des allèles dans les reads qui les supportent, entre les allèles de référence et les allèles alternatifs. Voir un allèle seulement près des extrémités des reads indique une erreur, car c'est là que les séquenceurs ont tendance à faire le plus d'erreurs. Cependant, certaines variations situées près des bords des régions séquencées seront nécessairement couvertes par les extrémités des lectures, donc il n'est pas possible de simplement définir un seuil absolu de «distance minimale à partir de la fin de la lecture». C'est pourquoi un test de somme de rang est utilisé pour évaluer s'il y a une différence dans la façon dont l'allèle de référence et l'allèle alternatif sont supportés. Le résultat idéal est une valeur proche de zéro, ce qui indique qu'il y a peu ou pas de différence dans la localisation des allèles par rapport à la fin des reads. Une valeur négative indique que l'allèle alternatif se trouve aux extrémités des reads plus souvent que l'allèle de référence. Inversement, une valeur positive indique que l'allèle de référence se trouve plus souvent à l'extrémité des reads que l'allèle alternatif.    
+ **Root Mean Square of the mapping quality of reads across all samples (MQ)** : Cette annotation fournit une estimation de la qualité de cartographie globale des reads prenant en charge une varaitions. Il produit à la fois des données brutes (somme des carrés et nombre de reads totaux) et le carré moyen calculé. Les données brutes sont utilisées pour calculer avec précision le carré moyen en combinant plusieurs échantillons.   
+ **MappingQualityRankSum Rank Sum Test for mapping qualities of REF versus ALT reads (MQRankSum)**: Cette annotation compare les qualités de cartographie des reads supportant l'allèle de référence avec celles supportant l'allèle alternatif. Le résultat idéal est une valeur proche de zéro, ce qui indique qu'il y a peu ou pas de différence. Une valeur négative indique que les reads supportant l'allèle alternatif ont des scores de qualité de cartographie inférieurs à ceux supportant l'allèle de référence. Inversement, une valeur positive indique que les reads supportant l'allèle alternatif ont des scores de qualité de cartographie plus élevés que ceux supportant l'allèle de référence. Cette annotation peut être utilisée pour évaluer la confiance d'une variation et est une covariable recommandée pour le recalibrage de variante (VQSR). Trouver une différence de qualité statistiquement significative suggère que le processus de séquençage et / ou de cartographie peut avoir été biaisé ou affecté par un artefact. En pratique, seules les valeurs négatives faibles sont filtrées lors de l'évaluation de la qualité des variants car l'idée est de filtrer les variants pour lesquels la qualité des données supportant l'allèle alternatif est relativement faible.        
+ **Inbreeding Coefficient (InbreedingCoeff)**: Cette annotation évalue s'il existe des preuves de consanguinité dans une population. Plus le score est élevé, plus le risque de consanguinité est élevé. Le calcul est une généralisation continue du test de Hardy-Weinberg pour le déséquilibre.